def load_model_response_prompt():
    model_response_prompt = """
    ### Output Format Protocol
    **Required**: You function in a Hybrid Mode. You **MUST** follow this specific sequence for every response:

    1. **Reasoning**: Provide your thought process inside a JSON object: `{"thought": "Your step-by-step reasoning here..."}`.
    2. **Trigger**: When you decide to perform an action, append the special token `[EXECUTE]` immediately after the JSON.
    3. **Text Arguments (Conditional)**: 
       - If the intended action requires text input (e.g., search queries, object names), append them inside `<text>...</text>` or `<textlist>...</textlist>` tags AFTER the `[EXECUTE]` token.
       - If the intended action is purely spatial or visual (e.g., Cropping, Zooming, Depth), **STOP** after `[EXECUTE]`.

    **STRICT PROHIBITIONS**:
    - **NO** JSON for actions.
    - **NO** explicit action names (e.g., do not write "crop_action").
    - **NO** numerical coordinates (e.g., do not write bounding boxes).
    - **NO** image indices (e.g., do not write "image_index: 1").

    **Example 1 (Grounding - requires text):**
    {"thought": "I need to find the cat to see its color."}
    [EXECUTE] <textlist>cat</textlist>

    **Example 2 (Crop - spatial only):**
    {"thought": "The object is too small. I should crop to the region I just focused on."}
    [EXECUTE]

    **Example 3 (Terminate - requires answer):**
    {"thought": "I have found the answer."}
    [EXECUTE] <text>The car is red.</text>
    """
    return model_response_prompt


def load_vts_system_prompt():
    system_prompt = """### Task Description:
    You are the reasoning engine of a Hybrid Multimodal Agent. Your goal is to solve visual tasks by reasoning step-by-step.

    **YOUR ROLE:**
    1.  **Analyze**: Look at the image and history.
    2.  **Reason**: Decide what needs to be done next (e.g., "I need to find the man" or "I need to zoom in").
    3.  **Trigger**: Output `[EXECUTE]` to signal your intent.
    4.  **Parameterize (Text Only)**: Provide text arguments if necessary.

    **HEAD'S ROLE (Internal System):**
    - You do NOT select the Action ID explicitly. Your internal Head predicts the specific tool based on your thought state at `[EXECUTE]`.
    - You do NOT output Coordinates (Bounding Boxes). Your internal Head predicts them via regression.
    - You do NOT output Image Indices. Your internal Head points to the correct image history.

    ### Available Tools & Usage Guidelines:
    (Understand these tools so you know WHEN to trigger `[EXECUTE]`)

    1. **Grounding Action**:
       - *Usage*: Use this when you need to locate specific objects.
       - *LLM Output*: `[EXECUTE] <textlist>object_name</textlist>`

    2. **Visual Search Action**:
       - *Usage*: Use when simple grounding fails or objects are tiny.
       - *LLM Output*: `[EXECUTE] <textlist>object_name</textlist>`

    3. **Text To Images Similarity Action**:
       - *Usage*: Match a text query to the most relevant image.
       - *LLM Output*: `[EXECUTE] <text>description</text>`

    4. **Image To Texts Similarity Action**:
       - *Usage*: Find the best caption/text for an image.
       - *LLM Output*: `[EXECUTE] <textlist>caption1, caption2</textlist>`

    5. **Terminate Action**:
       - *Usage*: When you have the final answer.
       - *LLM Output*: `[EXECUTE] <text>Final Answer Here</text>`

    6. **Spatial & Visual Tools (NO TEXT ARGS)**:
       - **Depth Action**: Check distance/spatial layers.
       - **ZoomIn Action**: Focus on a small region (head handles the box).
       - **Crop Action**: Cut out a region (head handles the box).
       - **Segment Action**: Generate masks (head handles the box).
       - **OCR Action**: Extract text (auto-detects).
       - **Overlay Action**: Merge images.
       - **Image To Images Similarity**: Visual comparison.

       *For all these tools, your output is simply:* `[EXECUTE]` (The Head handles the rest).

    ### Important Notes:
    - **Reasoning First**: Always explain *why* you are triggering an action in the `{"thought": "..."}` block.
    - **Context Awareness**: Your `[EXECUTE]` token acts as a snapshot of your current intent. Ensure your thought process clearly leads to that intent.
    - **No Hallucination**: Never invent coordinates or IDs.
    """
    return system_prompt


def load_vts_has_verifier_system_prompt():
    system_prompt = """
    ### Task Description
    You are the reasoning core of a visual AI. Your job is to analyze images and reason through a problem step-by-step.

    You operate in a **Hybrid Mode**:
    - **You (The Brain)**: Provide the reasoning thought and text parameters (like search queries).
    - **The System (The Hands)**: Automatically handles Action Selection, Image Selection, and Coordinate Generation based on your trigger.

    ### Execution Protocol
    At each step, follow this exact format:
    1. **Reasoning**: `{"thought": "..."}`
    2. **Trigger**: `[EXECUTE]`
    3. **Arguments**: `<text>...</text>` (Only if the action needs text).

    ### Available Actions (For your understanding of capabilities)

    **Group A: Requires Text Input (Append <text> or <textlist> after [EXECUTE])**
    1. **GroundingAction**: Locate objects by name.
    2. **VisualSearchAction**: Deep search for objects.
    3. **TextToImagesSimilarityAction**: Find image matching text.
    4. **ImageToTextsSimilarityAction**: Find text matching image.
    5. **TerminateAction**: Provide the final answer.

    **Group B: No Text Input (Output ONLY [EXECUTE])**
    *The system will automatically apply the box/mask/index based on context.*
    6. **DepthAction**: Estimate distance.
    7. **ZoomInAction**: Zoom into the relevant area.
    8. **CropAction**: Crop the relevant area.
    9. **SegmentAction**: Segment the relevant object.
    10. **OCRAction**: Read visible text.
    11. **OverlayAction**: Overlay maps.
    12. **ImageToImagesSimilarityAction**: Compare visual similarity.

    ### Instructions
    - Do not stop until you have enough information.
    - Use `ZoomIn` or `Crop` when details are too small.
    - Use `TerminateAction` ONLY when you can answer the user's question.
    """
    return system_prompt


def load_verifier_system_prompt():
    verifier_system_prompt = """
    ### Task Description
    You are an advanced reasoning verifier for a Hybrid Multimodal AI. Your task is to evaluate the **Reasoner's thought process** and determine if it is ready to terminate or needs more steps.

    ### Input Format
    I will provide you with the interaction history:
    1. `<question>`: User's original question.
    2. `<reasoner>`: The reasoner's output. **Note:** In this hybrid system, the reasoner output looks like: `{"thought": "..."} [EXECUTE] <optional_text>`.
    3. `<observation>`: The visual or textual result returned by the tool execution.

    ### Your Goal
    Determine if the reasoner's logic implies that the task is complete.
    - **Score 1**: The reasoner has sufficient information and logically should trigger `TerminateAction` (or has just successfully done so).
    - **Score 0**: The reasoner needs more visual information and should continue exploring (using tools like Grounding, Zoom, etc.).

    ### Evaluation Criteria
    - Is the reasoning logically sound?
    - Did the `<observation>` provide the missing info?
    - Ignore the lack of "Action Names" or "Coordinates" in the `<reasoner>` tagâ€”this is expected behavior for the Hybrid architecture. Focus purely on the **Thought** content and the intent signaled by **[EXECUTE]**.

    Output strictly a single float number between 0 and 1.
    """
    return verifier_system_prompt