# Updated Qwen2.5-VL Agent Configuration
# ✅ Includes all new configurable parameters

model_id: "/root/autodl-tmp/dissertation/Qwen/Qwen2___5-VL-3B-Instruct"

# Training Configuration
epochs: 3

# ✅ NEW: LM Loss Weight (balance between language modeling and agent head)
# Higher value = more focus on language generation
# Lower value = more focus on agent tasks
lm_loss_weight: 1.0

# ✅ NEW: Backbone Training Strategy
# true = freeze backbone, only train agent head (faster, less memory)
# false = train both backbone and head (better performance, more resources)
freeze_backbone: true

# ✅ NEW: Gradient Checkpointing
# true = save memory by recomputing gradients (slower but uses less VRAM)
# false = faster training but uses more VRAM
gradient_checkpointing: true

# Agent Head Configuration
agent_config:
  # Note: hidden_size and visual_hidden_size will be auto-synced from model
  # These values are just defaults and will be overwritten
  hidden_size: 1536        # Will match model's actual hidden size
  visual_hidden_size: 1280 # Will match model's visual encoder size
  
  num_actions: 13          # Number of available actions (0-12)
  max_history_images: 10   # Maximum images to keep in history
  max_boxes: 5             # Maximum bounding boxes per prediction
  num_heads: 8             # Number of attention heads in decoder
  dropout: 0.1             # Dropout rate

# LoRA Configuration (Parameter-Efficient Fine-Tuning)
lora:
  enabled: true
  r: 16                    # LoRA rank (higher = more parameters, better fit)
  alpha: 32                # LoRA scaling factor (usually 2*r)
  dropout: 0.05            # LoRA dropout
  target_modules:          # Which modules to apply LoRA to
    - "q_proj"             # Query projection
    - "k_proj"             # Key projection  
    - "v_proj"             # Value projection
    - "o_proj"             # Output projection
    - "gate_proj"          # FFN gate projection
    - "up_proj"            # FFN up projection
    - "down_proj"          # FFN down projection

# Alternative: Full Fine-Tuning Configuration (commented out)
# Uncomment to train full model instead of using LoRA
# lora:
#   enabled: false

# Training Tips:
# 
# 1. For 24GB GPU (single):
#    - Use LoRA with r=16
#    - Keep freeze_backbone=true
#    - Enable gradient_checkpointing
#    - DeepSpeed Stage 2
#
# 2. For 24GB GPU (4x multi-GPU):
#    - Use LoRA with r=32 or full fine-tuning
#    - Keep freeze_backbone=false for better performance
#    - Enable gradient_checkpointing
#    - DeepSpeed Stage 3 with offloading
#
# 3. For 80GB GPU (A100):
#    - Full fine-tuning recommended
#    - freeze_backbone=false
#    - gradient_checkpointing=false
#    - DeepSpeed Stage 2 or 3
#
# 4. Loss Weight Tuning:
#    - If agent head learns too slowly: increase lm_loss_weight to 0.5-0.8
#    - If language quality degrades: increase lm_loss_weight to 1.5-2.0
#    - Standard balanced training: lm_loss_weight=1.0
